{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Quadro K4000 (CNMeM is enabled with initial size: 75.0% of memory, cuDNN 5004)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FIELD_SIZE = (3, 3)\n",
    "SIZE_OF_LINE_TO_WIN = 3\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self, field_size, size_of_line_to_win):\n",
    "        self.field = np.zeros(field_size, dtype = np.int8)\n",
    "        self.field_size = self.field.shape\n",
    "        self.size_of_line_to_win = size_of_line_to_win\n",
    "        self.move = 1\n",
    "    \n",
    "    def make_move(self, action):\n",
    "        \"\"\"\n",
    "            action is tuple (row, col)\n",
    "        \"\"\"\n",
    "        self.field[action] = self.move\n",
    "        self.move = -self.move\n",
    "        return abs(self.check_win_condition())\n",
    "        \n",
    "    def avaliable_moves(self):\n",
    "        \"\"\"\n",
    "            Avaliable moves array of (row, col)\n",
    "        \"\"\"\n",
    "        return zip(*np.where(self.field == 0))\n",
    "    \n",
    "    def players(self):\n",
    "        return [-1, 1]\n",
    "    \n",
    "    def player(self):\n",
    "        return self.move\n",
    "    \n",
    "    def finish(self): \n",
    "        return self.check_win_condition() != 0 or np.all(self.field != 0)\n",
    "    \n",
    "    def action_as_array(self, action):\n",
    "        res = np.zeros((1, self.field_size[0], self.field_size[1]), dtype = np.float32)\n",
    "        res[:, action[0], action[1]] = 1\n",
    "        return res\n",
    "    \n",
    "    def state_as_array(self):\n",
    "        return np.expand_dims(np.array([self.field == -self.move, self.field == 0, self.field == self.move],\n",
    "                                       dtype = np.float32), axis = 0)\n",
    "    \n",
    "    def check_win_condition(self):\n",
    "        \"\"\"\n",
    "            If someone allready win, return winer, otherwise zero\n",
    "        \"\"\"\n",
    "        ### For every submaxtix of size (SIZE_OF_LINE_TO_WIN, SIZE_OF_LINE_TO_WIN) check if there is a winner\n",
    "        for i in range(self.field_size[0] - self.field_size[0] + 1):\n",
    "            for j in range(self.field_size[1] - self.field_size[1] + 1):\n",
    "                sub_matrix = self.field[i:(i + self.size_of_line_to_win),j:(j + self.size_of_line_to_win)]\n",
    "                #If any equal to SIZE_OF_LINE_TO_WIN we have a winner\n",
    "                #check rows\n",
    "                row_sum = sub_matrix.sum(axis = 1)\n",
    "                #check columns\n",
    "                col_sum = sub_matrix.sum(axis = 0)\n",
    "                #check diagonals\n",
    "                diag_sum = np.array([np.diag(sub_matrix).sum(), np.diag(np.fliplr(sub_matrix)).sum()])\n",
    "                all_sum = np.hstack([row_sum, col_sum, diag_sum])\n",
    "                \n",
    "                if np.any(all_sum == self.size_of_line_to_win):\n",
    "                    return 1\n",
    "                if np.any(all_sum == -self.size_of_line_to_win):\n",
    "                    return -1\n",
    "        return 0\n",
    "\n",
    "    \n",
    "    def __str__(self):\n",
    "        res = np.empty(FIELD_SIZE, dtype = object)\n",
    "        res[self.field == 0] = \"_\"\n",
    "        res[self.field == 1] = \"X\"\n",
    "        res[self.field == -1] = \"0\"\n",
    "        return \"\\n\".join(map(lambda x: \"\".join(x), res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TicTacToeStrategy:\n",
    "    def __init__(self, field_size):\n",
    "        self.field_size = field_size\n",
    "        self.state = T.tensor4(\"state\", dtype='float32')\n",
    "        self.action = T.tensor3(\"action\", dtype='float32')\n",
    "        self.reward = T.vector(\"reward\", dtype='float32')\n",
    "    \n",
    "    def get_net(self):        \n",
    "        state_input = lasagne.layers.InputLayer(shape = (None, 3, FIELD_SIZE[0], FIELD_SIZE[1]),\n",
    "                                        input_var = self.state, name = 'state')\n",
    "        action_input = lasagne.layers.InputLayer(shape = (None, FIELD_SIZE[0], FIELD_SIZE[1]),\n",
    "                                         input_var = self.action, name = 'action')\n",
    "        action_reshaped = lasagne.layers.ReshapeLayer(action_input, shape = (-1, 1, FIELD_SIZE[0], FIELD_SIZE[1]))\n",
    "\n",
    "        net = lasagne.layers.concat([state_input, action_reshaped], axis = 1)\n",
    "\n",
    "        net = lasagne.layers.DenseLayer(net, num_units = 60, name='hiden1')\n",
    "\n",
    "        return lasagne.layers.DenseLayer(net, num_units = 1,\n",
    "                        name='output', nonlinearity=lasagne.nonlinearities.identity)\n",
    "    \n",
    "    def compile(self):\n",
    "        \"\"\"\n",
    "            Compile a network, return theano function for training from triplets (state, action, reward)\n",
    "            and for predicting from (state, action)\n",
    "        \"\"\"\n",
    "        self.net = self.get_net()\n",
    "        reward_predicted = lasagne.layers.get_output(self.net).reshape((-1, ))\n",
    "        self.all_weights = lasagne.layers.get_all_params(self.net)\n",
    "        loss = lasagne.objectives.squared_error(reward_predicted, self.reward).mean()\n",
    "        updates_sgd = lasagne.updates.sgd(loss, self.all_weights, 0.1)\n",
    "        self.train_fun = theano.function([self.state, self.action, self.reward], loss, updates = updates_sgd)\n",
    "        self.predict_fun = theano.function([self.state, self.action], reward_predicted)\n",
    "    \n",
    "    def predict_rewards(self, game, actions, batch_size = 10):\n",
    "        rewards_predicted = np.empty(len(actions), dtype = np.float32)\n",
    "        for begin in xrange(0, len(actions), batch_size):\n",
    "            end = min(len(actions), begin + batch_size)\n",
    "            state_batch = np.vstack([game.state_as_array() for _ in actions[begin:end]])\n",
    "            action_batch = np.vstack([game.action_as_array(action) for action in actions[begin:end]])\n",
    "            rewards_predicted[begin:end] = self.predict_fun(state_batch, action_batch)\n",
    "        return rewards_predicted    \n",
    "\n",
    "    def train_on_random_batch(self, states, actions, rewards, batch_size = 10):\n",
    "        index = np.random.choice(len(rewards), min(batch_size, len(rewards)), replace = False)\n",
    "        return self.train_fun(states[index], actions[index], rewards[index])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "class Trainer:\n",
    "    def __init__(self, game_factory, strategy, epsilon_policy = lambda i_game: 0.5, future_rewards_decay = 0.9,\n",
    "                 batch_size = 25, replay_memory_size = 100, n_games = 10001):\n",
    "        \n",
    "        self.strategy = strategy\n",
    "        self.strategy.compile()\n",
    "        self.game_factory = game_factory \n",
    "        \n",
    "        game = self.game_factory()\n",
    "        game_state_shape = game.state_as_array().shape\n",
    "        action_state_shape = game.action_as_array(game.avaliable_moves()[0]).shape\n",
    "        \n",
    "        self.replay_states = np.empty([replay_memory_size] + list(game_state_shape)[1:], dtype = np.float32)\n",
    "        self.replay_actions = np.empty([replay_memory_size] + list(action_state_shape)[1:], dtype = np.float32)\n",
    "        self.replay_rewards = np.empty([replay_memory_size], dtype = np.float32)\n",
    "        \n",
    "        self.replay_memory_size = replay_memory_size\n",
    "        self.replay_next_index = 0\n",
    "        self.replays_in_memory = 0\n",
    "        self.epsilon_policy = epsilon_policy\n",
    "        self.n_games = n_games\n",
    "        self.gamma = future_rewards_decay\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def chose_action(self, game, epsilon):\n",
    "        actions = game.avaliable_moves()\n",
    "        rewards_predicted = self.strategy.predict_rewards(game, actions, self.batch_size)\n",
    "        probs_action = [epsilon / len(rewards_predicted)] * len(rewards_predicted)\n",
    "        probs_action[np.argmax(rewards_predicted)] += (1 - epsilon)  \n",
    "        i_action = np.random.choice(range(len(actions)), 1, p = probs_action)\n",
    "        return actions[i_action]\n",
    "    \n",
    "    def store_in_replay_memory(self, state, action, reward):        \n",
    "        self.replay_states[self.replay_next_index] = state\n",
    "        self.replay_actions[self.replay_next_index] = action\n",
    "        self.replay_rewards[self.replay_next_index] = reward\n",
    "        self.replays_in_memory += 1\n",
    "        self.replays_in_memory = min(self.replays_in_memory, self.replay_memory_size)\n",
    "        self.replay_next_index = (self.replay_next_index + 1) % self.replay_memory_size\n",
    "        \n",
    "    def play_random_replays(self):        \n",
    "        return self.strategy.train_on_random_batch(self.replay_states[:self.replays_in_memory],\n",
    "                                                   self.replay_actions[:self.replays_in_memory],\n",
    "                                                   self.replay_rewards[:self.replays_in_memory],\n",
    "                                                   self.batch_size)\n",
    "    \n",
    "    def train(self, verbose = True):\n",
    "        loss = 0\n",
    "        for i_game in xrange(self.n_games):\n",
    "            game = self.game_factory()\n",
    "            epsilon = self.epsilon_policy(i_game)\n",
    "\n",
    "            finished = False\n",
    "            while not finished:\n",
    "                action = self.chose_action(game, epsilon)\n",
    "                state_memory, action_memory = game.state_as_array(), game.action_as_array(action)\n",
    "                reward_memory = game.make_move(action)\n",
    "                finished = game.finish()\n",
    "                if not finished:                    \n",
    "                    reward_memory -= self.gamma * np.max(self.strategy.predict_rewards(game, game.avaliable_moves()))\n",
    "                \n",
    "                self.store_in_replay_memory(state_memory, action_memory, reward_memory)\n",
    "                loss += self.play_random_replays()\n",
    "                \n",
    "            if i_game % 50 == 0 and verbose:        \n",
    "                print \"Game number %s, loss is %s, epsilon %s\" % (i_game, loss / 50, epsilon)\n",
    "                loss = 0\n",
    "                clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number 3150, loss is 0.808865172789, epsilon 0.724980335958\n"
     ]
    }
   ],
   "source": [
    "strategy = TicTacToeStrategy(field_size = FIELD_SIZE)\n",
    "epsilon_policy = lambda i_game: max(0.3, 0.99 * (0.99) ** (i_game / 100))\n",
    "trainer = Trainer(lambda: TicTacToe(FIELD_SIZE, SIZE_OF_LINE_TO_WIN), strategy, epsilon_policy=epsilon_policy)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Bot:\n",
    "    def __init__(self, strategy):\n",
    "        self.strategy = strategy\n",
    "    \n",
    "    def get_next_action(self, game):\n",
    "        moves = game.avaliable_moves()\n",
    "        rewards = strategy.predict_rewards(game, moves, 10)\n",
    "        return moves[np.argmax(rewards)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomBot:\n",
    "    def get_next_action(self, game):\n",
    "        index = np.random.choice(len(game.avaliable_moves()))\n",
    "        return game.avaliable_moves()[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "players = {1 : RandomBot(), -1 : Bot(strategy)}\n",
    "game = TicTacToe(FIELD_SIZE, SIZE_OF_LINE_TO_WIN)\n",
    "while not game.finish():\n",
    "    action = players[game.player()].get_next_action(game)\n",
    "    game.make_move(action)\n",
    "    print game\n",
    "    clear_output(wait = True)\n",
    "    time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
